{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jDg6qj25gT2l",
   "metadata": {
    "id": "jDg6qj25gT2l"
   },
   "source": [
    "# This is based off of some of the baseline inference scripts that Lawrence ran for full sentence holdings. Here I run it for parenthetical holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JzYp-VLD42Cs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzYp-VLD42Cs",
    "outputId": "01801b01-ba2f-484a-827b-e65d09b7b7c4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install textwrap\n",
    "!pip install langchain\n",
    "!pip install gc\n",
    "!pip install torch\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sAoWUaju51AQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAoWUaju51AQ",
    "outputId": "f62b93a3-55f9-40b9-ff79-84676b0f2c50"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate # I got an error when loadin the model that this is required when loading the model\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffb1dd-f870-4379-8eef-88ddc792541e",
   "metadata": {
    "id": "55ffb1dd-f870-4379-8eef-88ddc792541e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import textwrap\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0979e-d803-43ae-b372-df5dd72bc355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0b0979e-d803-43ae-b372-df5dd72bc355",
    "outputId": "10f5df3c-6f6b-48ba-d2c3-0338f17f58b8"
   },
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "HUGGINGFACEHUB_API_TOKEN = \"<token>\" #load from hf_token.txt\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8D1gDfvu-rtl",
   "metadata": {
    "id": "8D1gDfvu-rtl"
   },
   "outputs": [],
   "source": [
    "# This code was borrowed from another notebook, as I needed a quick fix to using an accelerator: https://brev.dev/blog/fine-tuning-llama-2-your-own-data\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DuKwM8Xo_pjB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuKwM8Xo_pjB",
    "outputId": "104c46b3-fbf7-43c6-ba3e-050ad55fecfa"
   },
   "outputs": [],
   "source": [
    "# This is new\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AcJhQipt_xBP",
   "metadata": {
    "id": "AcJhQipt_xBP"
   },
   "outputs": [],
   "source": [
    "# This is new\n",
    "file_path_test = '/content/drive/MyDrive/Lang Gen Project/qlora_data/cleaned_test_qlora.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59adc15-db39-4fbd-a8d2-486cb44056d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fe841826f6ba4982ba4dddeb481a9b86",
      "c27425233df147d3acbca228765c79ae",
      "1ee4a7e2a78a407c8047ac544ba91284",
      "75c6e2e03a36469384dd3304b1cfd62e",
      "a45bc86cd5804c498fd491379a9edc4a",
      "c1acdb4c748b4e059eff0ab419c9b6ce",
      "e159eeec8723459693fbbf937a50b8ca",
      "ea7ec548e8404ab999f7dcd5866b0569",
      "662fc953cb544fbba17d50f508947525",
      "0511b5f422084db78b80ca69375b9e58",
      "6a9f6671ba1f43eaa38a85d3e97fcd3f"
     ]
    },
    "id": "b59adc15-db39-4fbd-a8d2-486cb44056d1",
    "outputId": "0287c06a-5dd9-48f5-9b20-e063e5f29048"
   },
   "outputs": [],
   "source": [
    "model_directory = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             # load_in_8bit=True,\n",
    "                                             load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a579b9e-84e1-424a-8774-251060a33fef",
   "metadata": {
    "id": "6a579b9e-84e1-424a-8774-251060a33fef"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 1024,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1531e1d-6199-4eac-b758-56047f7ec343",
   "metadata": {
    "id": "d1531e1d-6199-4eac-b758-56047f7ec343"
   },
   "source": [
    "# The cell below has important functions\n",
    "## It's best to run it, regardless of which task and model you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe50231-e60c-4233-b32e-15e729c042c0",
   "metadata": {
    "id": "fbe50231-e60c-4233-b32e-15e729c042c0"
   },
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=1024,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n",
    "\n",
    "def count_words(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "def summarize_chunks(chunks, model, tokenizer):\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        output = llm_chain.run(chunk)\n",
    "        # print(count_words(output))\n",
    "        # parse_text(output)\n",
    "        summaries.append(output)\n",
    "    return summaries\n",
    "\n",
    "def create_final_summary(summaries):\n",
    "    # Option 1: Just join the summaries\n",
    "    final_summary = ' '.join(summaries)\n",
    "\n",
    "    # Option 2: Apply another round of summarization (can be useful for coherence)\n",
    "    # final_summary = generate(final_summary)  # This is recursive and might degrade quality\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "def chunk_text_with_overlap(text, chunk_word_count, overlap_word_count):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        current_chunk_end = index + chunk_word_count\n",
    "\n",
    "        # We don't want to overshoot the list of words for the current chunk\n",
    "        current_chunk_end = min(current_chunk_end, len(words))\n",
    "\n",
    "        chunk = \" \".join(words[index:current_chunk_end])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        index += chunk_word_count - overlap_word_count\n",
    "\n",
    "        # If the calculated index doesn't advance (due to large overlap), we force it to advance to avoid an infinite loop\n",
    "        if index >= current_chunk_end:\n",
    "            index = current_chunk_end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to load data from the JSON file and extract the desired information.\n",
    "def load_and_extract_data(file_path):\n",
    "    # Reading the file.\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Parsing the JSON data.\n",
    "\n",
    "    for o in data[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "        if o[\"type\"] == \"majority\":\n",
    "            return o[\"text\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def save_summary_to_text(summary, output_folder, file_path, condensed=False):\n",
    "    \"\"\"\n",
    "    Save the content of 'summary' to a text file derived from the name of the input file.\n",
    "    \"\"\"\n",
    "    # Extract the base file name without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    if condensed:\n",
    "        summary_file_name = f\"{base_name}_condensed_summary.txt\"\n",
    "    else:\n",
    "        summary_file_name = f\"{base_name}_summary.txt\"\n",
    "\n",
    "    summary_file_path = os.path.join(output_folder, summary_file_name)\n",
    "\n",
    "    try:\n",
    "        with open(summary_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary)\n",
    "        print(f\"Summary successfully written to {summary_file_name}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Unable to write to file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the content of a text file.\n",
    "\n",
    "    :param file_path: str, path to the file to read.\n",
    "    :return: str, content of the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6a9f7-0f4b-4f72-ae30-03f745738ea2",
   "metadata": {
    "id": "5af6a9f7-0f4b-4f72-ae30-03f745738ea2"
   },
   "source": [
    "# Parenthetical generation\n",
    "## - Using Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b2025-c56f-4aa1-9b3d-7213ed645391",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe1b2025-c56f-4aa1-9b3d-7213ed645391",
    "outputId": "ebbc03e9-7528-49ca-f650-4284130570d5"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "# Instruction and prompt slightly rephrased.\n",
    "instruction = \"Use the case document to extract the concise holding and phrase it as a parenthetical, which should look something like this: holding that the balance between costs and benefits comes out against applying the exclusionary rule in civil deportation hearings. {text}\"\n",
    "system_prompt = \"You are a legal expert who specializes in extracting accurate and concise parenthetical holdings from case documents. Give only the holdings, no other breakdowns or extra text.\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bvI32z13EAMJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvI32z13EAMJ",
    "outputId": "883cae2a-4aaa-4bf4-95ed-90842b067e8c"
   },
   "outputs": [],
   "source": [
    "# This is new\n",
    "print(\"Prompt is:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TGFy0wx4Fhtl",
   "metadata": {
    "id": "TGFy0wx4Fhtl"
   },
   "outputs": [],
   "source": [
    "# This is new\n",
    "test_df = pd.read_json(file_path_test, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y_duIKefOLSJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_duIKefOLSJ",
    "outputId": "6faae6c4-0970-4ad8-b12b-87e42cb72c15"
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mGpG5KHeLgGN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGpG5KHeLgGN",
    "outputId": "13cc041e-16cf-45fd-ddad-f1c0ef5f5eba"
   },
   "outputs": [],
   "source": [
    "test_input = test_df.iloc[0][\"input\"]\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V4g8KZmvOteE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4g8KZmvOteE",
    "outputId": "d09ce882-0d26-4e3d-f59c-2ec180467f9c"
   },
   "outputs": [],
   "source": [
    "test_input_reference = test_df.iloc[0][\"output\"]\n",
    "print(test_input_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "to3xs61JL4V1",
   "metadata": {
    "id": "to3xs61JL4V1"
   },
   "outputs": [],
   "source": [
    "test_output = llm_chain.run(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sD4y_dT-Mivv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sD4y_dT-Mivv",
    "outputId": "8bc6111a-2ead-4bc6-b635-ac83451b3c2b"
   },
   "outputs": [],
   "source": [
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1Eu6zsXJZ1",
   "metadata": {
    "id": "de1Eu6zsXJZ1"
   },
   "outputs": [],
   "source": [
    "test_df1 = test_df.drop(13)\n",
    "test_df1 = test_df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HWnx6f66N5-k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWnx6f66N5-k",
    "outputId": "bc0a00cd-046a-47c0-86b1-1f12c13127ef"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"Input\", \"Prediction\", \"Reference\"])\n",
    "num_nulls = 0\n",
    "\n",
    "for i in range(len(test_df1)):\n",
    "  print(f\"Predicting on input number: {i}\")\n",
    "  input_txt = test_df.iloc[i][\"input\"]\n",
    "  # output_txt = llm_chain.run(input_txt)\n",
    "\n",
    "  try:\n",
    "    # Attempt to generate output\n",
    "    output_txt = llm_chain.run(input_txt)\n",
    "  except RuntimeError:\n",
    "    # If a RuntimeError occurs, use a default NULL value\n",
    "    print(\"Generation failed, inserting NULL value\")\n",
    "    output_txt = \"NULL\"\n",
    "    num_nulls += 1\n",
    "  reference_txt = test_df.iloc[i][\"output\"]\n",
    "\n",
    "  temp_df = pd.DataFrame({'Input': [input_txt], 'Prediction': [output_txt], 'Reference': [reference_txt]})\n",
    "\n",
    "  results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "print(\"Inference has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dntT96I-b2HT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dntT96I-b2HT",
    "outputId": "208d9121-52ae-498f-b2e3-bf9e5fa0e5ec"
   },
   "outputs": [],
   "source": [
    "print(f\"The number of null values inserted was {num_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9e0LXvUVo8u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9e0LXvUVo8u",
    "outputId": "7d44dca5-7fc1-4d3b-9e28-1062447b2e33"
   },
   "outputs": [],
   "source": [
    "check_input = test_df.iloc[13][\"input\"]\n",
    "print(check_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVE0aF84WOYa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVE0aF84WOYa",
    "outputId": "c576c8b3-511f-4bdf-c95c-fe5abf9517f9"
   },
   "outputs": [],
   "source": [
    "word_count = len(check_input.split())\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LmLFe9LZRE2t",
   "metadata": {
    "id": "LmLFe9LZRE2t"
   },
   "outputs": [],
   "source": [
    "out_path = \"/content/drive/MyDrive/Lang Gen Project/Results/llama2_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3lLl2eD1Q7OX",
   "metadata": {
    "id": "3lLl2eD1Q7OX"
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0511b5f422084db78b80ca69375b9e58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ee4a7e2a78a407c8047ac544ba91284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea7ec548e8404ab999f7dcd5866b0569",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_662fc953cb544fbba17d50f508947525",
      "value": 2
     }
    },
    "662fc953cb544fbba17d50f508947525": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a9f6671ba1f43eaa38a85d3e97fcd3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75c6e2e03a36469384dd3304b1cfd62e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0511b5f422084db78b80ca69375b9e58",
      "placeholder": "​",
      "style": "IPY_MODEL_6a9f6671ba1f43eaa38a85d3e97fcd3f",
      "value": " 2/2 [00:05&lt;00:00,  2.33s/it]"
     }
    },
    "a45bc86cd5804c498fd491379a9edc4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1acdb4c748b4e059eff0ab419c9b6ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c27425233df147d3acbca228765c79ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1acdb4c748b4e059eff0ab419c9b6ce",
      "placeholder": "​",
      "style": "IPY_MODEL_e159eeec8723459693fbbf937a50b8ca",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e159eeec8723459693fbbf937a50b8ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea7ec548e8404ab999f7dcd5866b0569": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe841826f6ba4982ba4dddeb481a9b86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c27425233df147d3acbca228765c79ae",
       "IPY_MODEL_1ee4a7e2a78a407c8047ac544ba91284",
       "IPY_MODEL_75c6e2e03a36469384dd3304b1cfd62e"
      ],
      "layout": "IPY_MODEL_a45bc86cd5804c498fd491379a9edc4a"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
